{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCbVP5O6j15h",
    "tags": []
   },
   "source": [
    "Background: Dr. D wants to invest his profits in peer2peerlending (https://www.lendingclub.com/). He decides to invest $10,000,000.\n",
    "\n",
    "Problem Statement: Propose and support with data an investment strategy for peer2peer lending.\n",
    "\n",
    "Data Supplied: 30 million records from the site, from 2007 to 2018. Two files: accepted and rejected loans\n",
    "\n",
    "Action items for notebook: all assumptions documented, Python code must be executable, upload any additional data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4Olow4gq7JN",
    "outputId": "3f7fa68b-cd80-4927-a38d-52727f12ac01"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9oJu0XirAis",
    "outputId": "8644c762-225a-45d0-9bb0-c4927d14754f"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('accepted_2007_to_2018Q4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2u38wRe2mY_"
   },
   "source": [
    "## Here we begin our data cleaning. We will remove missing columns missing more than 50% of their values from the original \"data\", and dropping any loans that are missing more than 10 values in any of the features. The dataframe Loans will only have loans that either defaulted or were fully paid, as having current loans are not useful for making models because their outcome is not known yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrnlGYM62mZA"
   },
   "source": [
    "First, we create the \"Data_Not_Current\" set, which holds data for which the loan status is not Current, minus any missing rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nlRTfpgwrN-1"
   },
   "outputs": [],
   "source": [
    "data_missing_columns = data.columns[100*(data.isnull().sum()/len(data.index)) > 50]\n",
    "\n",
    "Data_Not_Current = data.drop(data_missing_columns, axis=1)\n",
    "\n",
    "Loans = Data_Not_Current.loc[(Data_Not_Current[\"loan_status\"] == \"Charged Off\") | (Data_Not_Current[\"loan_status\"] == \"Default\") | (Data_Not_Current[\"loan_status\"] == \"Fully Paid\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A844gaLo2mZA"
   },
   "source": [
    "Here we are cleaning Loans, which have some missing rows and columns removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LABbVbKlwH3Z"
   },
   "outputs": [],
   "source": [
    "Loans_missing_columns = Loans.columns[100*(Loans.isnull().sum()/len(Loans.index)) > 50]\n",
    "Loans = Loans.drop(Loans_missing_columns, axis=1)\n",
    "\n",
    "Loans_missing_rows = Loans[Loans.isnull().sum(axis=1) > 10].index\n",
    "Loans = Loans.drop(Loans_missing_rows, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value counts are printed out for the loan status, and in total the data set was reduced to around 1.25 million loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azSylMLVG-pI",
    "outputId": "e65daed9-7dbc-45d7-c537-40c1c6df810d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fully Paid     1019542\n",
       "Charged Off     258241\n",
       "Default             40\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loans['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b1tzfk_2mZC"
   },
   "source": [
    "### The following variables are either not available before a loan was given out, or had little to no effect on the models used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8EY6IoI6ifo",
    "outputId": "8e411422-881a-44cd-d3cf-8330697bfff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notice that the default rate is 20.2126 %.\n"
     ]
    }
   ],
   "source": [
    "variables_to_drop = [\"debt_settlement_flag\",\"last_pymnt_amnt\",\"recoveries\", \"collection_recovery_fee\", \"issue_d\", \"addr_state\", \"earliest_cr_line\", \"zip_code\", \"last_credit_pull_d\",\"emp_title\",\"hardship_flag\",\"policy_code\",\"pymnt_plan\",\"id\",\"url\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"tot_coll_amt\", \"tot_cur_bal\", \"avg_cur_bal\",\"last_pymnt_d\"]\n",
    "Loans = Loans.drop(variables_to_drop,axis=1)\n",
    "variables_to_drop2=[\"grade\", \"sub_grade\",\"emp_length\", \"funded_amnt\", \"funded_amnt_inv\", \"fico_range_high\", \"fico_range_low\", \"last_fico_range_high\", \"last_fico_range_low\"]\n",
    "Loans = Loans.drop(variables_to_drop2,axis=1)\n",
    "counts = Loans['loan_status'].value_counts()\n",
    "default_rate = (counts[1]+counts[2])/(counts[0]+counts[1]+counts[2])\n",
    "\n",
    "print(\"Notice that the default rate is\", round(default_rate*100, 4), \"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G63P3mnz2mZD"
   },
   "source": [
    "After exploring the data, it was found that there were a couple ten thousand different options for the title feature. This causes an issue when trying to one hot encode the variables, so to combat this, the top four titles were found which are stored in the preserved variable, and the rest were changed to other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nuDi-HplG-pL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Loans['title'].value_counts()\n",
    "preserved = ['Debt consolidation','Credit card refinancing','Home improvement','Major purchase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here term was changed to as a whole to be an int so it is machine readable, and a couple of other variables that only had two options were changed to 0 and 1 instead of the previous labels they had so they could be used in regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tPtmZ5HbG-pL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Loans.loc[~Loans['title'].isin(preserved), 'title'] = 'Other'\n",
    "Loans['term'] = Loans.term.str.replace(' months','').astype(int)\n",
    "Loans['disbursement_method'] = Loans['loan_status'].apply(lambda x: 0 if x=='DirectPay' else 1)\n",
    "Loans['application_type'] = Loans['application_type'].apply(lambda x: 0 if x=='Individual' else 1)\n",
    "Loans['initial_list_status'] = Loans['initial_list_status'].apply(lambda x: 0 if x=='w' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the calculation for what would be the expected profit of the loan, which is the interest of the loan, assumming it is payed in full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>expected_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3600.0</td>\n",
       "      <td>36</td>\n",
       "      <td>13.99</td>\n",
       "      <td>123.03</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>...</td>\n",
       "      <td>76.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>178050.0</td>\n",
       "      <td>7746.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>13734.0</td>\n",
       "      <td>1</td>\n",
       "      <td>829.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24700.0</td>\n",
       "      <td>36</td>\n",
       "      <td>11.99</td>\n",
       "      <td>820.28</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>small_business</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>97.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>314017.0</td>\n",
       "      <td>39475.0</td>\n",
       "      <td>79300.0</td>\n",
       "      <td>24667.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4830.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>10.78</td>\n",
       "      <td>432.66</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>218418.0</td>\n",
       "      <td>18696.0</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>14877.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5959.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10400.0</td>\n",
       "      <td>60</td>\n",
       "      <td>22.45</td>\n",
       "      <td>289.91</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>104433.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>major_purchase</td>\n",
       "      <td>Major purchase</td>\n",
       "      <td>...</td>\n",
       "      <td>96.6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>439570.0</td>\n",
       "      <td>95768.0</td>\n",
       "      <td>20300.0</td>\n",
       "      <td>88097.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6994.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11950.0</td>\n",
       "      <td>36</td>\n",
       "      <td>13.44</td>\n",
       "      <td>405.18</td>\n",
       "      <td>RENT</td>\n",
       "      <td>34000.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16900.0</td>\n",
       "      <td>12798.0</td>\n",
       "      <td>9400.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2636.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  term  int_rate  installment home_ownership  annual_inc  \\\n",
       "0     3600.0    36     13.99       123.03       MORTGAGE     55000.0   \n",
       "1    24700.0    36     11.99       820.28       MORTGAGE     65000.0   \n",
       "2    20000.0    60     10.78       432.66       MORTGAGE     63000.0   \n",
       "4    10400.0    60     22.45       289.91       MORTGAGE    104433.0   \n",
       "5    11950.0    36     13.44       405.18           RENT     34000.0   \n",
       "\n",
       "  verification_status loan_status             purpose               title  \\\n",
       "0        Not Verified  Fully Paid  debt_consolidation  Debt consolidation   \n",
       "1        Not Verified  Fully Paid      small_business               Other   \n",
       "2        Not Verified  Fully Paid    home_improvement               Other   \n",
       "4     Source Verified  Fully Paid      major_purchase      Major purchase   \n",
       "5     Source Verified  Fully Paid  debt_consolidation  Debt consolidation   \n",
       "\n",
       "   ...  pct_tl_nvr_dlq  percent_bc_gt_75  pub_rec_bankruptcies  tax_liens  \\\n",
       "0  ...            76.9               0.0                   0.0        0.0   \n",
       "1  ...            97.4               7.7                   0.0        0.0   \n",
       "2  ...           100.0              50.0                   0.0        0.0   \n",
       "4  ...            96.6              60.0                   0.0        0.0   \n",
       "5  ...           100.0             100.0                   0.0        0.0   \n",
       "\n",
       "   tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n",
       "0         178050.0             7746.0          2400.0   \n",
       "1         314017.0            39475.0         79300.0   \n",
       "2         218418.0            18696.0          6200.0   \n",
       "4         439570.0            95768.0         20300.0   \n",
       "5          16900.0            12798.0          9400.0   \n",
       "\n",
       "   total_il_high_credit_limit  disbursement_method  expected_profit  \n",
       "0                     13734.0                    1           829.08  \n",
       "1                     24667.0                    1          4830.08  \n",
       "2                     14877.0                    1          5959.60  \n",
       "4                     88097.0                    1          6994.60  \n",
       "5                      4000.0                    1          2636.48  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loans['expected_profit'] = (Loans['installment'] *  (Loans['term']) - Loans['loan_amnt'])\n",
    "Loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some features were removed because they were missing a lot of data, there is still a lot of missing data in the dataframe, which will cause issues later when the data is used in the Random Forest and other models. An imputer is used to fill in the values, with the numeric features replacing any missing values with the median, and the categorical features having missing data be replaced with a designation of Other. Imputing allows the data to be used later, because otherwise there would issues with the Random Forest, Adaptive Boost, and the Gradient Boosted Tree which will be seen later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jvg8SXvaHpya",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "target = 'loan_status'\n",
    "numeric_features = Loans.select_dtypes(include=['int64', 'float64']).columns\n",
    "tmp_annual_inc = Loans['annual_inc']\n",
    "categorical_features = Loans.select_dtypes(include=['object', 'category']).drop([target], axis=1).columns\n",
    "for col in categorical_features.tolist():\n",
    "        Loans[col] = Loans[col].astype('category')\n",
    "Loans[target] = Loans[target].astype('category')\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "imputer = imputer.fit(Loans[numeric_features.tolist()])\n",
    "Loans[numeric_features.tolist()] = imputer.transform(Loans[numeric_features.tolist()])\n",
    "\n",
    "imputercat = SimpleImputer(missing_values = np.nan, strategy = 'constant', fill_value='Missing')\n",
    "Loans[categorical_features] = imputercat.fit_transform(Loans[categorical_features.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one feature that was added and it is looking at the installment payed monthly over the annual income. There were a lot of values in the annual income column that were 0, which could either mean that that person did not input the income, or that their income is actually 0. To combat that value from being infinity, one was added to the annual income, and while it may technically make the feature innaccurate, the point of adding it is to try and help some of the models with accuracy. Other feature were added as well, but there was no real significant difference in any of the accuracies, so none were kept in the final product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loans['installment/ monthly income'] = (Loans['installment']/(1+(Loans['annual_inc']/12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a random sample is taken from the cleaned dataframe. This is to increase the speed of the Random Forest and other models. There were attempts with the full dataframe, and other larger sample sizes, but the difference in accuracy was negligible, if at all increased, and it allows for more of the variables to be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loans_Sample = Loans.sample(75000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to split the data into test and train dataframes in order for it to be used in the models. The X dataframe has all of the feature variables, while the y dataframe only has the target variables. In the X dataframe, all of the categorical variables were one hot encoded so that they were machine readable. Lastly, the response variable was recoded to have the Fully Paid Variables be assigned to 1, and the Defaulting Loans be set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = Loans_Sample.loc[:, Loans.columns != target]\n",
    "y = Loans_Sample[target]\n",
    "X = pd.get_dummies(X)\n",
    "repmap={\"Fully Paid\": 1, \"Default\": 0, \"Charged Off\":0}\n",
    "y.replace(repmap, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following functions are for graphing and were provided in the class python notebooks showing the accuracies, precision, f1 score, and other metrics for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def viz_conf_mat(y_test, y_pred):\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  con_matrix = confusion_matrix(y_test, y_pred)\n",
    "  confusion_matrix_df = pd.DataFrame(con_matrix, ('Default', 'Paid'), ('Default', 'Paid'))\n",
    "  heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\", cmap=\"Blues\")\n",
    "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "  plt.ylabel('Actual', fontsize = 14)\n",
    "  plt.xlabel('Predicted', fontsize = 14)\n",
    "    \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "def baseline_AUC(y_test):\n",
    "    ones = [1 for _ in y_test]\n",
    "    fpr, tpr, _ = roc_curve(y_test, ones)\n",
    "    results = {'fpr': fpr, 'tpr': tpr,\n",
    "      'accuracy': accuracy_score(y_test, ones),\n",
    "      'recall': recall_score(y_test, ones),\n",
    "      'precision': precision_score(y_test, ones),\n",
    "      'roc AUC': 0.5}\n",
    "    return {'Baseline' : results}\n",
    "\n",
    "def model_AUC(AUC, y_pred, y_test, probs, label):\n",
    "    \"\"\"Computes statistics and shows ROC curve compared to prior methods.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    AUC[label] = {'fpr': fpr, 'tpr': tpr,\n",
    "      'accuracy': accuracy_score(y_test, y_pred),\n",
    "      'recall': recall_score(y_test, y_pred),\n",
    "      'precision': precision_score(y_test, y_pred),\n",
    "      'roc AUC': roc_auc_score(y_test, probs) }\n",
    "    return AUC\n",
    "\n",
    "def plot_AUC(AUC):\n",
    "    metrics = ['accuracy', 'recall', 'precision', 'roc AUC']\n",
    "    print(' '*20, end ='')\n",
    "    for m in metrics:\n",
    "      print(f'{m.capitalize():>10}', end = '')\n",
    "    print()\n",
    "    for label, results in AUC.items():\n",
    "        print(f'{label:>18}:', end = '')\n",
    "        for m in metrics:\n",
    "          print(f'{results[m]:10.2f}', end = '')\n",
    "        print()\n",
    "    \n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    for label, results in AUC.items():\n",
    "      plt.plot(results['fpr'], results['tpr'], label = label)\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "    plt.show()\n",
    "def plot_importances(model, X_test, y_test, mycolumns):\n",
    "    importances = pd.Series(model.feature_importances_, index = mycolumns)\n",
    "    permImp = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    sorted_importances = importances.sort_values()\n",
    "    sorted_importances = sorted_importances[-15:]\n",
    "    sorted_idx = permImp.importances_mean.argsort()\n",
    "    sorted_idx = sorted_idx[-15:]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(35,15))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sorted_importances.plot(kind='barh', color='skyblue');\n",
    "    plt.title(\"Feature Importance (train set)\") \n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(\n",
    "        permImp.importances[sorted_idx].T,\n",
    "        vert=False, \n",
    "        labels=np.array(mycolumns)[sorted_idx],\n",
    "    )\n",
    "    plt.title(\"Permutation Importance (test set)\")\n",
    "    plt.show()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def fit_evaluate(model, X_train, y_train, X_test, y_test, \n",
    "                 label='', AUC=dict(), mycolumns=X.columns,\n",
    "                 show_confusion=True, show_Imp=True):\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "   \n",
    "  if (show_confusion):\n",
    "     viz_conf_mat(y_test, y_pred)\n",
    "  \n",
    "  if (len(AUC) > 0): \n",
    "    AUC = model_AUC(AUC, y_pred, y_test, model.predict_proba(X_test)[:, 1], label)\n",
    "    plot_AUC(AUC)\n",
    "\n",
    "  if (show_Imp):\n",
    "    plot_importances(model, X_test, y_test, mycolumns)\n",
    "\n",
    "  return AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model used was the Gradient Boosting Classifier. Some of the hyperparameters were adjusted, but there was no major difference in any of the changes as well. Overall the AUC for this model was usually around 95%, while the F1 score for the defaults was around .77-.80. The other important thing to note is that this model used both of the FICO score ranges as the most important feature. As it will be seen later, this is a common theme between each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "\n",
    "# Split data into 70% train and 30% test, stratified by target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
    "\n",
    "# Instantiate a GradientBoostingRegressor 'gbt'\n",
    "gbt = GradientBoostingClassifier(n_estimators=150, max_depth=1, random_state=42)\n",
    "\n",
    "gbt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "GB_RMSE = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "fit_evaluate(gbt, X_test, y_test,X_test, y_test, \n",
    "                       label='GBT', AUC = baseline_AUC(y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am pretty sure this can just be deleted, but I will keep it as Markdown for now just to be safe until the entire thing is ran again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model is the Random Forest. The accuracies and F1 scores are pretty similar to the GBT, and it also generally had an AUC of around 95%. The important thing to note once again is that the most important features were the FICO score ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "\n",
    "# Split data into 70% train and 30% test, stratified by target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
    "\n",
    "# Instantiate a random forests classifier\n",
    "rf = RandomForestClassifier(n_estimators=25, bootstrap = True, max_features = 'auto', \n",
    "                            min_samples_leaf = 5, criterion='gini', random_state=42, max_depth=15)\n",
    "\n",
    "results = fit_evaluate(rf, X_train, y_train, X_test, y_test, \n",
    "                       label='Random Forest', AUC = baseline_AUC(y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model that was used was the Adaptive Boost. This usually had a lower F1 score compared to the other two, but did have an AUC of around 95%. It once again had the FICO score ranges consistently being the most important feature that it used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "idiot = DecisionTreeClassifier(max_depth=1, criterion='gini', min_samples_leaf = 5, splitter = \"random\")\n",
    "\n",
    "adb_clf = AdaBoostClassifier(base_estimator=idiot, n_estimators=25, learning_rate=.2, random_state=42)\n",
    "\n",
    "results = fit_evaluate(adb_clf, X_train, y_train, X_test, y_test, label='AdaBoost', AUC = results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest was used as a classifier, but the probabilities that it used for a loan being payed or not can be extracted. These probabilities were extracted and put into a DataFrame to be used later, with a probability of 1 being that the Random Forest thinks the loan is absolutely going to be payed, and a probability of 0 being the Random Forest thinking that the loan will absolutely not be paid and default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "probs = rf.predict_proba(X_test)[:, 1]\n",
    "results2= pd.DataFrame({'classes':y_pred, 'probabilities':probs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These probabilities were placed into a copy of the Test data, where it was then used to calculate the expected loss. This was calculated by doing 1 minus the calculated probability of the loan by the total amount of the loan. The expected loss was then subtracted from the previously calculated expected profit of the loan, which is the interest that would be payed if the loan was payed in full, which gave a new value which may be positive or negative. It was also assumed here that a defaulting loan would mean everything is lost from the loan amount given out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = X_test.copy()\n",
    "tmp['Prob of Paying Loan'] = results2['probabilities'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['expected_loss'] = (1-tmp['Prob of Paying Loan'])*tmp['loan_amnt'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['EP - EL'] = tmp['expected_profit'] - tmp['expected_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the newly calculated value from above, below multiple different method were used and the total profits were recorded, which was the 'EP - EL' column from above summed. The first method was just the top 100 values with the 'EP - EL' column sorted, and this was used to see what was the max profit that could be achieved in this example. The other methods included a random sample, sorting by the top expected profits, the top fico scores, highest interest rates and lowest interest rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_amnt(pm):\n",
    "    return np.sum(pm['loan_amnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "our_method = tmp.sort_values('EP - EL', ascending=False).head(100)\n",
    "our_method_value = calc_amnt(our_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_method = tmp.sample(100)\n",
    "random_method_value = calc_amnt(random_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ep = tmp.sort_values('expected_profit', ascending=False).head(100)\n",
    "top_ep_value = calc_amnt(top_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fico_score = tmp.sort_values('last_fico_range_high', ascending=False).head(100)\n",
    "fico_score_value = calc_amnt(fico_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_rate_high = tmp.sort_values('int_rate', ascending=False).head(100)\n",
    "int_rate_high_value = calc_amnt(int_rate_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_rate_low = tmp.sort_values('int_rate', ascending=True).head(100)\n",
    "int_rate_low_value = calc_amnt(int_rate_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the amount of money given may have been different in each of the methods, the ROI was calculated instead of net profit so they could be more accurately compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_method_profit = 100*(np.sum(our_method['EP - EL']) / our_method_value)\n",
    "random_method_profit = 100*(np.sum(random_method['EP - EL']) / random_method_value)\n",
    "top_ep_profit = 100*(np.sum(top_ep['EP - EL']) / top_ep_value)\n",
    "fico_score_profit = 100*(np.sum(fico_score['EP - EL']) / fico_score_value)\n",
    "int_rate_high_profit = 100*(np.sum(int_rate_high['EP - EL']) / int_rate_high_value)\n",
    "int_rate_low_profit = 100*(np.sum(int_rate_low['EP - EL']) / int_rate_high_value)\n",
    "d = {'Method': [\"Our Method\", \"Random Method\", \"Top Expected Profit\", \"Top Fico Scores\", \"Highest Interest Rates\", \"Lowest Interest Rates\"], 'ROI %': [our_method_profit, random_method_profit, top_ep_profit, fico_score_profit,int_rate_high_profit,int_rate_low_profit]}\n",
    "tab = pd.DataFrame(data=d, index=[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(14,14)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The barplot below shows the ROIs for each of the calculated methods. Because these values are based off of the probabilities from the Random Forest, which was not perfectly accurate, the real Return on Investmet from each of these methods may not be correct. What is important is the looking at what features seem to be doing better. The highest interest rates, and the top expected profit if a loan is payed in full seem to be pretty similar with the random sample and Top FICO scores doing a little worse. To see if there was a bias from the Random Sample, these features are going to be tested in a simualtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Method', y='ROI %', data=tab);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering the percentage of loans that actually defaulting from before, which was around 20%, the goal here to to make a dataframe with a much higher rate of defaulting loans, in order to make it harder to make money. This is accomplished by taking all of the defaulting loans and combining it with a small sample of the loans that were paid in full. The split here is roughly 85% deafulting loans, and roughly 15% loans that are actually going to be paid. This makes it much harder to actually have a positive return, because there are a lot more defaulting loans. While it is possible for a defaulting loan to give the lender a profit, in this simulation all of the loans that defaulted will be a loss of money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loans['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Defs = Loans.loc[(Loans[\"loan_status\"] == \"Charged Off\") | (Loans[\"loan_status\"] == \"Default\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paid_Loans = Loans.loc[(Loans[\"loan_status\"] == \"Fully Paid\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paid_Loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paid_Loans = Paid_Loans.sample(42000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [Paid_Loans, Defs]\n",
    "Bad_Sample = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bad_Sample['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bad_Sample['Simulated Profit Var'] = Bad_Sample['loan_status'].apply(lambda x: 1 if x=='Fully Paid' else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bad_Sample['Simulated Profit'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bad_Sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the calculation to find what percentage of the loan someone paid on average before the person defaulted. The calcualted value was around 54%, and this will be used to calculate how much money is lost from a loan that defaulting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = data.loc[(data[\"loan_status\"] == \"Charged Off\") | (data[\"loan_status\"] == \"Default\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2['% payed'] = test2['total_pymnt'] / test2['loan_amnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_payed_from_default = (1-np.mean(test2['% payed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the simulated profit will be calculated, with a loan that is fully paid being the interest on a loan, and a defaulting loan being the calculated average payment left for a defaulted loan times the loan amount and times negative one to represent a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(300281):\n",
    "    if (Bad_Sample['Simulated Profit Var'].iloc[i] == 1):\n",
    "        Bad_Sample['Simulated Profit'].iloc[i] = Bad_Sample['expected_profit'].iloc[i]\n",
    "    else:\n",
    "        Bad_Sample['Simulated Profit'].iloc[i] = (Bad_Sample['loan_amnt'].iloc[i] * -1)*avg_payed_from_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_profit(sam):\n",
    "    return np.sum(sam['Simulated Profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_amnt_sim(sam):\n",
    "    return np.sum(sam['loan_amnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the methods create above, these methods are based off of the features that were seen, but retested in this dataframe where a loan is much more likely to default. The methods include a random sample, highest FICO score, lowest FICO score, highest interest rates, and highest expected profit. The return on investment was also calculated instead of net profit to account for differences in total loan amount given out from each of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_method = Bad_Sample.sample(250)\n",
    "random_method_value = calc_profit(random_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FICO_method = Bad_Sample.sort_values('last_fico_range_high', ascending=False).head(250)\n",
    "FICO_method_value = calc_profit(FICO_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FICO_low_method = Bad_Sample.sort_values('last_fico_range_high', ascending=True).head(250)\n",
    "FICO_low_method_value = calc_profit(FICO_low_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_rate_high_method2 = Bad_Sample.sort_values('int_rate', ascending=False).head(250)\n",
    "int_rate_high_value = calc_profit(int_rate_high_method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_profit_method2 = Bad_Sample.sort_values('expected_profit', ascending=False).head(250)\n",
    "expected_profit_high_value = calc_profit(expected_profit_method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_profit = 100*(random_method_value / calc_amnt_sim(random_method))\n",
    "high_FICO_profit = 100*(FICO_method_value / calc_amnt_sim(FICO_method))\n",
    "low_FICO_profit = 100*(FICO_low_method_value / calc_amnt_sim(FICO_low_method))\n",
    "int_rate_high_profit = 100*(int_rate_high_value / calc_amnt_sim(int_rate_high_method2))\n",
    "expected_profit_profit = 100*(expected_profit_high_value / calc_amnt_sim(expected_profit_method2))\n",
    "\n",
    "d = {'Method': [\"Random Method\",\"Highest FICO Scores\",\"Lowest FICO Scores\", \"top interest rates\", \"top expected profit\"], 'ROI %': [random_profit, high_FICO_profit, low_FICO_profit, int_rate_high_profit, expected_profit_profit]}\n",
    "tabs = pd.DataFrame(data=d, index=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(14,14)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the methods, there is a glaring outlier out of the 5, with it being the highest FICO scores. All of the other method recorded big losses, which is not that unexpected knowing that 85% of these loans were expected to default and thus lose money in this simulation. The one outlier was the highest FICO scores, which was still able to have a positive return on investment, even with the increased defaulting loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Method', y='ROI %', data=tabs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because one trial could have some variability in the actually values, a function was created in order to run the simulation multiple times, but this time instead of 85% defaulting loans, it was increased to 90%, and there will be samples taken from both the defaulting loans and paid loans to make each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation():\n",
    "    Defs = Loans.loc[(Loans[\"loan_status\"] == \"Charged Off\") | (Loans[\"loan_status\"] == \"Default\")]\n",
    "    #print(np.mean(Defs['last_fico_range_high'])) this was used to find the average FICO score for the default loans 567\n",
    "    Defs = Defs.sample(4500)\n",
    "    Paid_Loans = Loans.loc[(Loans[\"loan_status\"] == \"Fully Paid\")]\n",
    "    #print(np.mean(Paid_Loans['last_fico_range_high'])) this was used to find the average FICO score for the paid loans which was 704\n",
    "    Paid_Loans = Paid_Loans.sample(500)\n",
    "    frames = [Paid_Loans, Defs]\n",
    "    Bad_Sample = pd.concat(frames)\n",
    "    avg_FICO = np.mean(Bad_Sample['last_fico_range_high'])\n",
    "    Bad_Sample['Simulated Profit Var'] = Bad_Sample['loan_status'].apply(lambda x: 1 if x=='Fully Paid' else -1)\n",
    "    Bad_Sample['Simulated Profit'] = 0\n",
    "    for i in range(5000):\n",
    "        if (Bad_Sample['Simulated Profit Var'].iloc[i] == 1):\n",
    "            Bad_Sample['Simulated Profit'].iloc[i] = Bad_Sample['expected_profit'].iloc[i]\n",
    "        else:\n",
    "            Bad_Sample['Simulated Profit'].iloc[i] = (Bad_Sample['loan_amnt'].iloc[i] * -1)*avg_payed_from_default\n",
    "    random_method = Bad_Sample.sample(250)\n",
    "    random_method_value = calc_profit(random_method)\n",
    "    FICO_method = Bad_Sample.sort_values('last_fico_range_high', ascending=False).head(250)\n",
    "    avg_FICO_from_top = np.mean(FICO_method['last_fico_range_high'])\n",
    "    FICO_method_value = calc_profit(FICO_method)\n",
    "    FICO_low_method = Bad_Sample.sort_values('last_fico_range_high', ascending=True).head(250)\n",
    "    FICO_low_method_value = calc_profit(FICO_low_method)\n",
    "    int_rate_high_method2 = Bad_Sample.sort_values('int_rate', ascending=False).head(250)\n",
    "    int_rate_high_value = calc_profit(int_rate_high_method2)\n",
    "    expected_profit_method2 = Bad_Sample.sort_values('expected_profit', ascending=False).head(250)\n",
    "    expected_profit_high_value = calc_profit(expected_profit_method2)\n",
    "    random_profit = 100*(random_method_value / calc_amnt_sim(random_method))\n",
    "    high_FICO_profit = 100*(FICO_method_value / calc_amnt_sim(FICO_method))\n",
    "    low_FICO_profit = 100*(FICO_low_method_value / calc_amnt_sim(FICO_low_method))\n",
    "    int_rate_high_profit = 100*(int_rate_high_value / calc_amnt_sim(int_rate_high_method2))\n",
    "    expected_profit_profit = 100*(expected_profit_high_value / calc_amnt_sim(expected_profit_method2))\n",
    "    return random_profit, high_FICO_profit, low_FICO_profit, int_rate_high_profit, expected_profit_profit, avg_FICO_from_top, avg_FICO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = []\n",
    "hf = []\n",
    "lf = []\n",
    "irh = []\n",
    "ep = []\n",
    "fs = []\n",
    "ft = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    rp1, hf1, lf1, irh1, ep1, fs1, ft1 = simulation()\n",
    "    rp.append(rp1)\n",
    "    hf.append(hf1)\n",
    "    lf.append(lf1)\n",
    "    irh.append(irh1)\n",
    "    ep.append(ep1)\n",
    "    fs.append(fs1)\n",
    "    ft.append(ft1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_mean = np.mean(rp)\n",
    "hf_mean = np.mean(hf)\n",
    "lf_mean = np.mean(lf)\n",
    "irh_mean = np.mean(irh)\n",
    "ep_mean = np.mean(ep)\n",
    "\n",
    "fs_mean = np.mean(fs)\n",
    "ft_mean = np.mean(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = {'Method': [\"Random Method\",\"Highest FICO Scores\",\"Lowest FICO Scores\", \"top interest rates\", \"top expected profit\"], 'Average ROI % over 1000 Trials': [rp_mean, hf_mean, lf_mean, irh_mean, ep_mean]}\n",
    "tabs = pd.DataFrame(data=da, index=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(14,14)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to original trial, the Highest FICO scores method is the only method that was able to have a positive return with major losses in all of the other methods. This shows that the original trial was not an outlier due to sampling, and the difference going with the highest FICO scores is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Method', y='Average ROI % over 1000 Trials', data=tabs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average FICO score of the all of the Highest FICO score methods was \" + str(fs_mean))\n",
    "print(\"Average FICO score of the total simulation dataset was \" + str(ft_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result from these findings, the investment method that should be followed is to look for the highest FICO scores abailable when giving out loans. In the total dataset, the average FICO score from Fully Paid loans was roughly 704, compared to 567 from the defaulting loans. Targeting high FICO scores, especially those above 750, is a viable investment strategy that can help to both minimize risk, and have potential for modest profit."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
